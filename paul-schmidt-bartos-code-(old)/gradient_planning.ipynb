{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import paule\n",
    "import paule.paule\n",
    "from paule import util\n",
    "from paule.util import (speak, normalize_cp, inv_normalize_cp, normalize_mel_librosa, inv_normalize_mel_librosa, stereo_to_mono, librosa_melspec, pad_same_to_even_seq_length, RMSELoss, mel_to_sig, pad_batch_online)\n",
    "from paule.models import *\n",
    "from matplotlib import cm\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "DIR = os.getcwd()\n",
    "DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/home/tino/ml_cloud_nextcloud/Common_Voice_data/\"\n",
    "RESULT_DICT = \"results/\"\n",
    "SAVE_DICT = \"saves/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Paule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add continue data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = pd.read_pickle(os.path.join(DATA_DIR, \"common_voice_geco_words_test_subset_slim_prot4.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(23082022)\n",
    "valid_sample = random.sample(range(len(valid)), 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['melspec_norm_synthesized'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m continue_data \u001b[38;5;241m=\u001b[39m \u001b[43mvalid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid_sample\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvector\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcp_norm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmelspec_norm_synthesized\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/core/frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3510\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3511\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3513\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5782\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5784\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5786\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:5845\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5842\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5844\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 5845\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['melspec_norm_synthesized'] not in index\""
     ]
    }
   ],
   "source": [
    "continue_data = valid.iloc[valid_sample][[\"vector\", \"cp_norm\", \"melspec_norm_synthesized\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_data[\"segment_data\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "paule_model = paule.paule.Paule(use_somatosensory_feedback=False, continue_data=None, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = pd.read_pickle(DATA_DIR + \"lexical_embedding_vectors.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_pickle(DATA_DIR + \"common_voice_geco_words_test_subset_slim_prot4.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Flac Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,row in test_data.iterrows():\n",
    "    sr = 48000\n",
    "    sig = row.wav_rec\n",
    "    file_name = row.file_name\n",
    "    label = row.label\n",
    "    \n",
    "    path = RESULT_DICT + label + \"/\"\n",
    "    if os.path.isdir(path):\n",
    "        file = path +  file_name + \"_\" + label + \".flac\"\n",
    "        if os.path.isfile(file):\n",
    "            j = 1\n",
    "            while os.path.isfile(file): \n",
    "                file = path +  file_name + \"_\" + label + \"%d.flac\" % j\n",
    "                j+=1\n",
    "        sf.write(file ,sig, sr)\n",
    "    else:\n",
    "        os.mkdir(path)\n",
    "        sf.write(path +  file_name + \"_\" + label + \".flac\" ,sig, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Shuffle Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for dic in os.listdir(RESULT_DICT):\n",
    "    if (\".DS_Store\" in dic) or (\"results\" in dic) : # .DS_Store stores custom attributes of its containing folder in macOS\n",
    "        continue\n",
    "    else:\n",
    "        if not os.path.isdir(os.path.join(SAVE_DICT, dic)):\n",
    "            os.mkdir(os.path.join(SAVE_DICT, dic))\n",
    "        path = os.path.join(RESULT_DICT, dic)\n",
    "        for file in os.listdir(path):\n",
    "            if (\".DS_Store\" in file) or (\"._\" in file) or \".flac\" not in file:\n",
    "                continue\n",
    "            else:\n",
    "                file = os.path.join(path,file)\n",
    "                files.append(file)\n",
    "random.seed(30112021)\n",
    "random.shuffle(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Omit already planned files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_planned_files = []\n",
    "for dic in os.listdir(SAVE_DICT):\n",
    "    if \".DS_Store\" in dic:\n",
    "        continue\n",
    "    else:\n",
    "        path = os.path.join(SAVE_DICT,dic)\n",
    "        if os.path.isdir(path):\n",
    "            for file in os.listdir(path):\n",
    "                if \"planned\" in file and \".flac\" in file and not \"best\" in file: \n",
    "                    planned_file = \"_\".join(file.split(\"_\")[:-1]) + \".flac\" \n",
    "                    already_planned_files.append(planned_file)\n",
    "\n",
    "unplanned_files = [file for file in files if file.split(\"/\")[-1] not in already_planned_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load predictive model already used and further learned during planning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load pred_model and pred_optimizer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pred_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mDEVICE)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/serialization.py:579\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    577\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    584\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "# load pred_model and pred_optimizer\n",
    "pred_model = torch.load(\"\", map_location=DEVICE)\n",
    "optimizer = torch.load(\"\", map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load inv_model and inv_optimizer\n",
    "inv_model = torch.load(\"\", map_location=DEVICE)\n",
    "inv_optimizer = torch.load(\"\", map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tube_model and tube_optimizer\n",
    "tube_model = torch.load(\"\", map_location=DEVICE)\n",
    "tube_optimizer = torch.load(\"\", map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paule_model.pred_model = pred_model\n",
    "paule_model.pred_optimizer = optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paule_model.inv_model = inv_model\n",
    "paule_model.inv_optimizer = inv_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paule_model.tube_model = tube_model\n",
    "paule_model.tube_optimizer = tube_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Planning and store results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize_title = 35\n",
    "fontsize_x = 30\n",
    "fontsize_y = 30\n",
    "fontsize_params = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_learning_inv = True\n",
    "for i, file in enumerate(unplanned_files):\n",
    "    i += len(already_planned_files)\n",
    "    target_acoustic = file #path + \"/\" + file\n",
    "    label = file.split(\"/\")[-1].split(\"_\")[-1][:-5]\n",
    "    target_semvec = vectors[vectors.label == label].vector.iloc[0]\n",
    "    \n",
    "    save_file = f\"{SAVE_DICT}/{os.path.dirname(file).split('/')[-1]}/{os.path.basename(file)[:-5]}\"\n",
    "    \n",
    "    results = paule_model.plan_resynth(learning_rate_planning=0.01,\n",
    "            learning_rate_learning=0.001,\n",
    "            learning_rate_learning_inv=0.001,\n",
    "            target_acoustic=target_acoustic,\n",
    "            target_semvec=target_semvec,\n",
    "            initialize_from=\"acoustic\",\n",
    "            objective=\"acoustic\",\n",
    "            n_outer=10, n_inner=25,\n",
    "            continue_learning=True,\n",
    "            continue_learning_inv=continue_learning_inv,\n",
    "            add_training_data_pred=False,\n",
    "            add_training_data_inv=True,\n",
    "            log_ii=1,\n",
    "            log_semantics=True,\n",
    "            n_batches=3, batch_size=8, n_epochs=10,\n",
    "            log_gradients=False,\n",
    "            plot=save_file, \n",
    "            seed=None,\n",
    "            verbose=True)\n",
    "    \n",
    "    # save model and optimizer\n",
    "    torch.save(paule_model.pred_model, f\"{save_file}_{str(i)}_pred_model.pt\")\n",
    "    torch.save(paule_model.pred_optimizer, f\"{save_file}_{str(i)}_pred_optimizer.pt\")\n",
    "    \n",
    "    if continue_learning_inv:\n",
    "        torch.save(paule_model.inv_model, f\"{save_file}_{str(i)}_inv_model.pt\")\n",
    "        torch.save(paule_model.inv_optimizer, f\"{save_file}_{str(i)}_inv_optimizer.pt\")\n",
    "    \n",
    "\n",
    "    # save results without model and optimizer\n",
    "    with open(f\"{save_file}.pkl\", 'wb') as pfile:\n",
    "        pickle.dump(results, pfile)\n",
    "        \n",
    "    # save continue data if used\n",
    "    if not paule_model.continue_data is None:\n",
    "        with open(f\"{SAVE_DICT}/continue_data.pkl\", 'wb') as pfile:\n",
    "            pickle.dump(paule_model.continue_data, pfile)\n",
    "    \n",
    "    \n",
    "    # save best synthesis acoustic\n",
    "    with open(f\"{save_file}_best_synthesis_acoustic.pkl\", 'wb') as pfile:\n",
    "        pickle.dump(paule_model.best_synthesis_acoustic, pfile)\n",
    "    pfile.close()\n",
    "    \n",
    "    prod_sig = paule_model.best_synthesis_acoustic.prod_sig\n",
    "    sf.write(save_file + \"_planned_best_acoustic.flac\", prod_sig, 44100)\n",
    "    del prod_sig\n",
    "    \n",
    "    if paule_model.best_synthesis_semantic:\n",
    "        # save results without model and optimizer\n",
    "        with open(f\"{save_file}_best_synthesis_semantic.pkl\", 'wb') as pfile:\n",
    "            pickle.dump(paule_model.best_synthesis_semantic, pfile)\n",
    "        pfile.close()\n",
    "        \n",
    "        prod_sig = paule_model.best_synthesis_semantic.prod_sig\n",
    "        sf.write(save_file + \"_planned_best_semantic.flac\", prod_sig, 44100)\n",
    "        del prod_sig\n",
    "    \n",
    "    if paule_model.use_somatosensory_feedback:\n",
    "        with open(f\"{save_file}_best_synthesis_somatosensory.pkl\", 'wb') as pfile:\n",
    "            pickle.dump(paule_model.best_synthesis_somatosensory, pfile)\n",
    "        pfile.close()\n",
    "        \n",
    "        prod_sig = paule_model.best_synthesis_somatosensory.prod_sig\n",
    "        sf.write(save_file + \"_planned_best_somatosensory.flac\", prod_sig, 44100)\n",
    "        del prod_sig\n",
    "\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "    # save initial and planned flac\n",
    "    prod_sr = results.prod_sr\n",
    "    sig_initial = results.initial_sig\n",
    "    sf.write(save_file + \"_initial.flac\", sig_initial, prod_sr)\n",
    "    prod_sig = results.prod_sig\n",
    "    sf.write(save_file + \"_planned.flac\", prod_sig, prod_sr)\n",
    "\n",
    "    # save svgs\n",
    "    planned_cp = results.planned_cp\n",
    "    path = save_file + '_svgs/'\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    util.export_svgs(util.inv_normalize_cp(planned_cp), path=path)\n",
    "    \n",
    "    # save svgs initial \n",
    "    initial_cp = results.initial_cp\n",
    "    path = save_file + '_svgs_initial/'\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    util.export_svgs(util.inv_normalize_cp(initial_cp), path=path)\n",
    "\n",
    "    # ffmpeg -r 80 -width 600 -i tract%05d.svg -i planned_0.flac planned_0.mp4\n",
    "    # /usr/bin/ffmpeg -r 80 -width 600 -i /home/tino/Documents/phd/projects/paule/results/000003-Wissenschaft_svgs/tract%05d.svg -i results/000003-Wissenschaft_planned.flac planned.mp4\n",
    "\n",
    "\n",
    "    # save loss plot\n",
    "    fig, ax = plt.subplots(figsize=(15, 8), facecolor=\"white\")\n",
    "    ax.plot(results.planned_loss_steps, label=\"planned loss\", c=\"C0\")\n",
    "    ax.legend()\n",
    "    fig.savefig(f\"{save_file}_loss.png\")\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 8), facecolor=\"white\")\n",
    "    ax.plot(results.prod_loss_steps, label=\"produced mel loss\", c=\"C1\")\n",
    "    ax.plot(results.planned_mel_loss_steps, label=\"planned mel loss\", c=\"C0\")\n",
    "    ax.legend()\n",
    "    fig.savefig(f\"{save_file}_loss_mel.png\")\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "    # save subloss plot\n",
    "    fig, ax = plt.subplots(figsize=(15, 8), facecolor=\"white\")\n",
    "    ax.plot(results.vel_loss_steps, label=\"vel loss\", c=\"C2\")\n",
    "    ax.plot(results.jerk_loss_steps, label=\"jerk loss\", c=\"C3\")\n",
    "    ax.legend()\n",
    "    fig.savefig(f\"{save_file}_loss_subloss.png\")\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "    # save semvec loss plot\n",
    "    fig, ax = plt.subplots(figsize=(15, 8), facecolor=\"white\")\n",
    "    ax.plot(results.pred_semvec_loss_steps, label=\"planned semvec loss\", c=\"C0\")\n",
    "    ax.plot(results.prod_semvec_loss_steps, label=\"produced semvec loss\", c=\"C1\")\n",
    "    ax.legend()\n",
    "    fig.savefig(f\"{save_file}_loss_semvec.png\")\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    \n",
    "    if paule_model.use_somatosensory_feedback:\n",
    "        # save tube loss plot\n",
    "        fig, ax = plt.subplots(figsize=(15, 8), facecolor=\"white\")\n",
    "        ax.plot(results.prod_tube_loss_steps, label=\"produced tube loss\", c=\"C0\")\n",
    "        ax.legend()\n",
    "        fig.savefig(f\"{save_file}_loss_tube.png\")\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(15, 8), facecolor=\"white\")\n",
    "        ax.plot(results.prod_tube_mel_loss_steps, label=\"produced tube mel loss\", c=\"C1\")\n",
    "        ax.plot(results.pred_tube_mel_loss_steps, label=\"planned tube mel loss\", c=\"C0\")\n",
    "        ax.legend()\n",
    "        fig.savefig(f\"{save_file}_loss_tube_mel.png\")\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(15, 8), facecolor=\"white\")\n",
    "        ax.plot(results.pred_tube_semvec_loss_steps, label=\"planned tube semvec loss\", c=\"C0\")\n",
    "        ax.plot(results.prod_tube_semvec_loss_steps, label=\"produced tube semvec loss\", c=\"C1\")\n",
    "        ax.legend()\n",
    "        fig.savefig(f\"{save_file}_loss_tube_semvec.png\")\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "    \n",
    "    \n",
    "    # save cps\n",
    "    # Vocal Tract CPs   \n",
    "    fig, ax = plt.subplots(facecolor=\"white\", figsize=(15, 10))\n",
    "    colors = [\"C%d\" % i for i in range(19)]\n",
    "\n",
    "    for i in range(4):\n",
    "        ax.plot(results.planned_cp[:, i], color=colors[i], ls = 'solid', lw = 2)\n",
    "        ax.plot(results.initial_cp[:, i], color=colors[i], ls = 'dotted', lw = 4)\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], color='black', ls=\"solid\", lw=2, label='Planned Cp'),#\n",
    "                       Line2D([0], [0], color='black',ls =\"dotted\", lw = 4 , label='Inital CP')]\n",
    "\n",
    "    ax.set_ylim((-1.1, 1.1))\n",
    "    plt.legend(handles=legend_elements, fontsize=fontsize_params, bbox_to_anchor=(1.0, 1),frameon = False) \n",
    "    ax.tick_params(axis='both', labelsize=fontsize_params)\n",
    "    ax.set_ylabel('Normalized Position' , fontsize=fontsize_y, labelpad=20)\n",
    "    ax.set_xlabel('Timestep (2.5ms)' , fontsize=fontsize_x, labelpad=20)  \n",
    "    plt.title(\"Vocal Tract Cps: '%s'\" % save_file.split(\"/\")[-1], fontsize=18, pad=10)\n",
    "    fig.savefig(f\"{save_file}_vocal_tract_cps.png\")\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    \n",
    "    # Glottis CPs\n",
    "    fig, ax = plt.subplots(facecolor=\"white\", figsize=(15, 10))\n",
    "    colors = [\"C%d\" % i for i in range(19)]\n",
    "\n",
    "    for i in range(4):\n",
    "        ax.plot(results.planned_cp[:, 19+i], color=colors[i], ls = 'solid', lw = 2)\n",
    "        ax.plot(results.initial_cp[:, 19+i], color=colors[i], ls = 'dotted', lw = 4)\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], color='black', ls=\"solid\", lw=2, label='Planned Cp'),#\n",
    "                       Line2D([0], [0], color='black',ls =\"dotted\", lw = 4 , label='Inital CP')]\n",
    "\n",
    "    ax.set_ylim((-1.1, 1.1))\n",
    "    plt.legend(handles=legend_elements, fontsize=fontsize_params, bbox_to_anchor=(1.0, 1),frameon = False) \n",
    "    ax.tick_params(axis='both', labelsize=fontsize_params)\n",
    "    ax.set_ylabel('Normalized Position' , fontsize=fontsize_y, labelpad=20)\n",
    "    ax.set_xlabel('Timestep (2.5ms)' , fontsize=fontsize_x, labelpad=20)\n",
    "    plt.title(\"Glottis Cps: '%s'\" % save_file.split(\"/\")[-1], fontsize=18, pad=10)\n",
    "    fig.savefig(f\"{save_file}_glottis_cps.png\")\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store final losses to txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = []\n",
    "with open(SAVE_DICT + '/results_loss.txt', 'w') as txt:\n",
    "    first_row = True\n",
    "    for dic in os.listdir(SAVE_DICT):\n",
    "        if \".DS_Store\" in dic:\n",
    "            continue\n",
    "        else:\n",
    "            path = os.path.join(SAVE_DICT,dic)\n",
    "            if os.path.isdir(path):\n",
    "                for file in os.listdir(path):\n",
    "                    if \".pkl\" in file and \"best_synthesis\" not in file: \n",
    "                        with open(os.path.join(path,file), 'rb') as f:\n",
    "                            results = pickle.load(f)\n",
    "                            file_name = \"_\".join(file.split(\"_\")[:-1])\n",
    "                            loss = results.planned_loss_steps[-1]\n",
    "                            prod_mel_loss = results.prod_loss_steps[-1]\n",
    "                            pred_mel_loss = results.planned_mel_loss_steps[-1]\n",
    "                            vel_loss = results.vel_loss_steps[-1]\n",
    "                            jerk_loss = results.jerk_loss_steps[-1]\n",
    "                            \n",
    "                            prod_semvec_loss = results.prod_semvec_loss_steps[-1]\n",
    "                            pred_semvec_loss = results.pred_semvec_loss_steps[-1] \n",
    "                            if hasattr(results, \"prod_tube_loss_steps\"):\n",
    "                                prod_tube_loss = results.prod_tube_loss_steps[-1]\n",
    "                                prod_tube_mel_loss = results.prod_tube_mel_loss_steps[-1]\n",
    "                                pred_tube_mel_loss = results.pred_tube_mel_loss_steps[-1]\n",
    "                                prod_tube_semvec_loss = results.prod_tube_semvec_loss_steps[-1]\n",
    "                                pred_tube_semvec_loss = results.pred_tube_semvec_loss_steps[-1]\n",
    "                        file_names.append(file_name)    \n",
    "                        if first_row:\n",
    "                            header = \"filename prod_mel_loss pred_mel_loss vel_loss jerk_loss prod_semvec_loss pred_semvec_loss\"\n",
    "                            if hasattr(results, 'prod_tube_loss_steps'):\n",
    "                                txt.write(header + \" prod_tube_loss prod_tube_mel_loss pred_tube_mel_loss prod_tube_semvec_loss pred_tube_semvec_loss\\n\") \n",
    "                            else:\n",
    "                                txt.write(header +\"\\n\")\n",
    "                            first_row = False\n",
    "                        if hasattr(results, 'prod_tube_loss_steps'):\n",
    "                            txt.write(file_name + \" %.5f %.5f %.5f %.5f %.5f %.5f %.5f %.5f %.5f %.5f %.5f\\n\" % (prod_mel_loss, pred_mel_loss, vel_loss, jerk_loss, prod_semvec_loss, pred_semvec_loss, prod_tube_loss, prod_tube_mel_loss, pred_tube_mel_loss, prod_tube_semvec_loss, pred_tube_semvec_loss))\n",
    "                        else:    \n",
    "                            txt.write(file_name + \" %.5f %.5f %.5f %.5f %.5f %.5f\\n\" % (prod_mel_loss, pred_mel_loss, vel_loss, jerk_loss, prod_semvec_loss, pred_semvec_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Model Loss for continued learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model_loss = []\n",
    "inv_model_loss = []\n",
    "tube_model_loss = []\n",
    "\n",
    "for dic in os.listdir(SAVE_DICT):\n",
    "    if \".DS_Store\" in dic:\n",
    "        continue\n",
    "    else:\n",
    "        path = os.path.join(SAVE_DICT,dic)\n",
    "        if os.path.isdir(path):\n",
    "            for file in os.listdir(path):\n",
    "                if \".pkl\" in file and \"best_synthesis\" not in file: \n",
    "                    with open(os.path.join(path,file), 'rb') as f:\n",
    "                        results = pickle.load(f)\n",
    "                        pred_model_loss.append(results.pred_model_loss)\n",
    "                        if hasattr(results, \"inv_model_loss\"):\n",
    "                            inv_model_loss.append(results.inv_model_loss)\n",
    "                        if hasattr(results, \"tube_model_loss\"):\n",
    "                            tube_model_loss.append(results.tube_model_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15,8), facecolor = \"white\")\n",
    "ax.plot(np.array(pred_model_loss).flatten(), label = \"pred_model loss\", c = \"C0\")\n",
    "#ax.plot(range(len(model_loss)),np.repeat(np.mean(model_loss),len(model_loss)) ,c = \"C1\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling with Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = pd.read_pickle(DATA_DIR + \"lexical_embedding_vectors.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 0\n",
    "for dic in os.listdir(SAVE_DICT):\n",
    "    if \".DS_Store\" in dic:\n",
    "        continue\n",
    "    else:\n",
    "        path = os.path.join(SAVE_DICT,dic)\n",
    "        if os.path.isdir(path):\n",
    "            for file in os.listdir(path):\n",
    "                if \".pkl\" in file and \"best_synthesis\" not in file: \n",
    "                    label = dic\n",
    "                    target_semvec = vectors[vectors.label == label].iloc[0].vector\n",
    "                    with open(os.path.join(path,file), 'rb') as f:\n",
    "                        results = pickle.load(f)\n",
    "                        file_name = \"_\".join(file.split(\"_\")[:-1])\n",
    "                        planned_cp = results.planned_cp\n",
    "                        initial_cp = results.initial_cp\n",
    "                        initial_sig = results.initial_sig\n",
    "                        initial_prod_mel = results.initial_prod_mel\n",
    "                        initial_pred_mel = results.initial_pred_mel\n",
    "                        target_sig = results.target_sig\n",
    "                        target_sr = results.target_sr\n",
    "                        target_mel = results.target_mel\n",
    "                        prod_sig = results.prod_sig\n",
    "                        prod_sr = results.prod_sr \n",
    "                        prod_mel = results.prod_mel\n",
    "                        pred_mel = results.pred_mel\n",
    "                        initial_prod_semvec = results.initial_prod_semvec\n",
    "                        initial_pred_semvec = results.initial_pred_semvec\n",
    "                        prod_semvec = results.prod_semvec\n",
    "                        pred_semvec = results.pred_semvec                      \n",
    "                        prod_mel_loss = results.prod_loss_steps\n",
    "                        planned_loss = results.planned_loss_steps\n",
    "                        planned_mel_loss = results.planned_mel_loss_steps\n",
    "                        vel_loss = results.vel_loss_steps\n",
    "                        jerk_loss = results.jerk_loss_steps\n",
    "                        prod_semvec_loss = results.prod_semvec_loss_steps\n",
    "                        pred_semvec_loss = results.pred_semvec_loss_steps\n",
    "                        prod_semvec_steps = results.prod_semvec_steps \n",
    "                        pred_semvec_steps = results.pred_semvec_steps\n",
    "                        pred_model_loss = results.pred_model_loss\n",
    "\n",
    "                        if hasattr(results, \"inv_model_loss\"):\n",
    "                            inv_model_loss = results.inv_model_loss\n",
    "\n",
    "                        if hasattr(results, \"tube_model_loss\"):\n",
    "                            initial_prod_tube = results.initial_prod_tube \n",
    "                            initial_pred_tube = results.initial_pred_tube \n",
    "                            initial_prod_tube_mel = results.initial_prod_tube_mel\n",
    "                            initial_pred_tube_mel = results.initial_pred_tube_mel\n",
    "                            prod_tube = results.prod_tube\n",
    "                            pred_tube = results.pred_tube\n",
    "                            prod_tube_mel = results.prod_tube_mel\n",
    "                            pred_tube_mel = results.pred_tube_mel\n",
    "                            initial_prod_tube_semvec = results.initial_prod_tube_semvec\n",
    "                            initial_pred_tube_semvec = results.initial_pred_tube_semvec\n",
    "                            prod_tube_semvec = results.prod_tube_semvec\n",
    "                            pred_tube_semvec = results.pred_tube_semvec\n",
    "                            prod_tube_loss = results.prod_tube_loss_steps\n",
    "                            pred_tube_mel_loss = results.pred_tube_mel_loss_steps\n",
    "                            prod_tube_mel_loss = results.prod_tube_mel_loss_steps\n",
    "                            pred_tube_semvec_loss = results.pred_tube_semvec_loss_steps\n",
    "                            prod_tube_semvec_loss = results.prod_tube_semvec_loss_steps\n",
    "                            prod_tube_semvec_steps = results.prod_tube_semvec_steps\n",
    "                            pred_tube_semvec_steps = results.pred_tube_semvec_steps\n",
    "                            tube_model_loss = results.tube_model_loss\n",
    "\n",
    "                        f.close()\n",
    "\n",
    "                        if ix == 0:\n",
    "                            columns = ['file_name','label', 'planned_cp', 'initial_cp', \n",
    "                                                  'initial_sig','initial_prod_mel', 'initial_pred_mel', \n",
    "                                                  'target_sig', 'target_sr','target_mel',\n",
    "                                                  'prod_sig','prod_sr','prod_mel', 'pred_mel', \n",
    "                                                  'initial_prod_semvec', 'initial_pred_semvec',\n",
    "                                                  'target_semvec',\n",
    "                                                  'prod_semvec', 'pred_semvec',\n",
    "                                                  'prod_mel_loss', 'planned_loss', 'planned_mel_loss', 'vel_loss', 'jerk_loss',\n",
    "                                                  'prod_semvec_loss', 'pred_semvec_loss',\n",
    "                                                  'prod_semvec_steps', 'pred_semvec_steps',\n",
    "                                                  'pred_model_loss']\n",
    "                            \n",
    "                            if hasattr(results, \"inv_model_loss\"):\n",
    "                                columns += [\"inv_model_loss\"]\n",
    "\n",
    "                            if hasattr(results, \"tube_model_loss\"):\n",
    "                                columns += [\"initial_prod_tube\", \"initial_pred_tube\", \"initial_prod_tube_mel\",\"initial_pred_tube_mel\",\n",
    "                                            \"prod_tube\", \"pred_tube\", \"prod_tube_mel\", \"pred_tube_mel\", \n",
    "                                             \"initial_prod_tube_semvec\", \"initial_pred_tube_semvec\", \n",
    "                                            \"prod_tube_semvec\", \"pred_tube_semvec\", \"prod_tube_loss\", \n",
    "                                            \"pred_tube_mel_loss\", \"prod_tube_mel_loss\", \"pred_tube_semvec_loss\",\n",
    "                                            \"prod_tube_semvec_loss\", \"prod_tube_semvec_steps\", \"pred_tube_semvec_steps\", \"tube_model_loss\"]\n",
    " \n",
    "                            \n",
    "                            final_results = pd.DataFrame(columns=columns)\n",
    "\n",
    "                    \n",
    "                        data = [file_name, label, planned_cp, initial_cp , \n",
    "                                initial_sig, initial_prod_mel, initial_pred_mel, \n",
    "                                target_sig, target_sr, target_mel,\n",
    "                                prod_sig, prod_sr, prod_mel, pred_mel, \n",
    "                                initial_prod_semvec, initial_pred_semvec,\n",
    "                                target_semvec, prod_semvec, pred_semvec,\n",
    "                                prod_mel_loss, planned_loss, planned_mel_loss, \n",
    "                                vel_loss, jerk_loss, prod_semvec_loss, pred_semvec_loss,\n",
    "                                prod_semvec_steps, pred_semvec_steps, pred_model_loss]\n",
    "                            \n",
    "                        if hasattr(results, \"inv_model_loss\"):\n",
    "                            data += [inv_model_loss]\n",
    "\n",
    "                        if hasattr(results, \"tube_model_loss\"):\n",
    "                            data += [initial_prod_tube, initial_pred_tube, initial_prod_tube_mel,initial_pred_tube_mel,\n",
    "                                     prod_tube, pred_tube, prod_tube_mel, pred_tube_mel, \n",
    "                                     initial_prod_tube_semvec, initial_pred_tube_semvec, \n",
    "                                     prod_tube_semvec, pred_tube_semvec, prod_tube_loss, \n",
    "                                     pred_tube_mel_loss, prod_tube_mel_loss, pred_tube_semvec_loss,\n",
    "                                     prod_tube_semvec_loss, prod_tube_semvec_steps, pred_tube_semvec_steps, tube_model_loss]\n",
    "            \n",
    "                        final_results.loc[ix] = data\n",
    "                        ix+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results.to_pickle(SAVE_DICT+\"/final_results.pkl\", protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove all individual results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dic in os.listdir(SAVE_DICT):\n",
    "    if \".DS_Store\" in dic:\n",
    "        continue\n",
    "    else:\n",
    "        path = os.path.join(SAVE_DICT,dic)\n",
    "        if os.path.isdir(path):\n",
    "            for file in os.listdir(path):\n",
    "                if \".pkl\" in file: \n",
    "                    os.remove(os.path.join(path,file))\n",
    "                if \".pt\" in file:\n",
    "                    if not int(file.split(\"_\")[-3]) == (len(final_results)-1):\n",
    "                        os.remove(os.path.join(path,file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
